"""
@author: Ming Ming Zhang, mmzhangist@gmail.com

RetinaNet Model
"""

# influenced by
# https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py

import tensorflow as tf

import resnet_fpn, subnets, losses, detections


def retinanet(
        mode,
        offsets_mean=None,
        offsets_std=None,
        architecture='resnet50', 
        train_bn=False, 
        channels_fmap=256,
        num_anchors_per_pixel=9, 
        num_object_classes=1,
        pi=0.01,
        alpha=0.25, 
        gamma=2.0,
        confidence_threshold=0.05, 
        num_top_scoring=1000,
        batch_size=2,
        max_objects_per_class_per_img=100,
        iou_threshold=0.5,
        output_top_scoring=False
        ):
    """
    Builds a RetinaNet.

    Parameters
    ----------
    mode : string
        The mode of building a retinanet either in 'training' or 'inference'.  
    offsets_mean, offsets_std : float
        The mean and std of anchor offsets for a given dataset. If offsets are 
        normalized, they will be used to de-normalize offsets.
    architecture : string, optional
        ResNet architecture in {'resnet50', 'resnet101'}. The default is 
        'resnet50'.
    train_bn : boolean, optional
        Whether one should normalize the layer input by the mean and variance 
        over the current batch. The default is False, i.e., use the moving
        average of mean and variance to normalize the layer input.
    channels_fmap : integer, optional
        The number of filters in all FPN conv layers. The default is 256.
    num_anchors_per_pixel : integer, optional
        The number of anchors to generate at different scales for every pixel;
        see anchors.anchors_from_fpn(). The default is 9.
    num_object_classes : integer, optional
        The number of classes containing only objects, i.e., object classes 
        denoted by positive integers while background denoted by 0. The default 
        is 1.
    pi : float, optional
        The bias initialization at the final conv layer of the classification
        subnet, prevents the large number of anchors from generating a large
        loss value in the first iteration of training. The default is 0.01.
    alpha : float, optional
        A weighting factor in [0,1] for the object class, addressing class 
        imbalance. The default is 0.25.
    gamma : float, optional
        A focusing parameter >= 0 for removing easy examples. The default is 
        2.0.
    confidence_threshold : float, optional
        The minimum selection's probabilites. The default is 0.05.
    num_top_scoring : integer, optional
        The number of top-scoring selections. The default is 1000.
    batch_size : integer, optional
        The batch size of input images. The default is 2.
    max_objects_per_class_per_img : integer, optional
        The maximum number of objects over all images for a particular class. 
        The default is 100.
    iou_threshold : float, optional
        An iou threshold for NMS. The default is 0.5.
    output_top_scoring : boolean, optional
        Whether to include the output of detections.select_top_scoring() in the
        inference mode. The default is False.

    Returns
    -------
    model : tf keras
        The retinanet.
            - Training mode 
                * inputs are a batch of images, anchor indicators, ground-truth 
                class ids and offsets generated by data_gen.data_generator(); 
                * outputs are predicted anchor probabilities, offsets, 
                classification and regression losses.
            - Inference mode 
                * inputs are a batch of raw images, a list of anchors at all 
                levels generated by anchors.anchors_from_fpn() and a window with 
                shape of [1, 4] used in clipping anchors in 
                detections.SelectTopScoring() where 4 is (y1, x1, y2, x2) corner 
                coordinates for all images in the batch.
                * outputs is a list of detections, each has corresponding target 
                boxes, class ids and scores.

    """
    assert mode in ['training', 'inference']
    
    # input images
    images = tf.keras.Input(shape=(None, None, 3), name='images')
        
    if mode == 'training':
        # inputs generated by anchors.anchors_targets()
        gt_anchor_indicators = tf.keras.Input(
            shape=(None,), 
            name='gt_anchor_indicators', 
            dtype=tf.int32)
        gt_anchor_class_ids = tf.keras.Input(
            shape=(None, num_object_classes), 
            name='gt_anchor_class_ids', 
            dtype=tf.int32)
        gt_anchor_offsets = tf.keras.Input(
            shape=(None, 4), 
            name='gt_anchor_offsets', 
            dtype=tf.float32)
    
    # backbone, ResNet + FPN
    fmaps = resnet_fpn.resnet_fpn(
        images, architecture, train_bn, channels_fmap)
    
    if mode == 'inference':
        # input generated by anchors.anchors_from_fpn(), and then each 
        # element is broadcasted to batch_size, resulting in shape of 
        # [batch_size, num_anchors_per_fmap, 4]
        anchors_fpn_batches = []
        for i in range(len(fmaps)):
            anchors_i = tf.keras.Input(
                shape=(None, 4), 
                name='anchors_p'+str(i+3), 
                dtype=tf.float32)
            anchors_fpn_batches.append(anchors_i)
        # input used when clipping anchors in detections.SelectTopScoring()
        window = tf.keras.Input(
            shape=(4), 
            batch_size=1, 
            name='window', 
            dtype=tf.int32)
    
    # classification and regression subnets
    cls_subnet = subnets.cls_subnet(
        num_anchors_per_pixel, 
        num_object_classes, 
        channels_fmap,
        pi)
    reg_subnet = subnets.reg_subnet(
        num_anchors_per_pixel, 
        channels_fmap)
    
    # outputs, list, each element is for one FPN level
    if mode == 'training':
        pred_anchor_probs, pred_anchor_offsets = [], []
    else:   
        list_anchor_idxes = []
        list_anchors, list_class_ids, list_scores = [], [], []
    
    # loop for each FPN level
    for i in range(len(fmaps)):
        # fmap, [batch_size, h_i, w_i, channels_fmap] where h_i and w_i denote
        # the current fmap size
        p = fmaps[i]
        # cls, [batch_size, h_i, w_i, num_anchors_per_pixel*num_object_classes]
        pred_anchor_probs_i = cls_subnet([p])
        # reshape, [batch_size, h_i*w_i*num_anchors_per_pixel, num_object_classes]
        pred_anchor_probs_i = tf.keras.layers.Reshape(
            (-1, num_object_classes), 
            name='cls_probs_p'+str(i+3)
            )(pred_anchor_probs_i)
        
        # reg, [batch_size, h_i, w_i, num_anchors_per_pixel*4]
        pred_anchor_offsets_i = reg_subnet([p])
        # reshape, [batch_size, h_i*w_i*num_anchors_per_pixel, 4]
        pred_anchor_offsets_i = tf.keras.layers.Reshape(
            (-1, 4), 
            name='reg_offsets_p'+str(i+3)
            )(pred_anchor_offsets_i)
        
        if mode == 'training':
            pred_anchor_probs.append(pred_anchor_probs_i)
            pred_anchor_offsets.append(pred_anchor_offsets_i)
        
        else:
            # filter low confidence, select top-scoring and refine anchors
            anchors_i = anchors_fpn_batches[i]
            select_top_scoring_inputs_i = [
                anchors_i, 
                pred_anchor_probs_i, 
                pred_anchor_offsets_i,
                window]
            select_top_scoring_outputs_i = detections.SelectTopScoring(
                confidence_threshold, 
                num_top_scoring,
                batch_size,
                offsets_mean,
                offsets_std,
                name='select_top_detection_p'+str(i+3)
                )(select_top_scoring_inputs_i)
            list_anchor_idxes.append(select_top_scoring_outputs_i[0])
            list_anchors.append(select_top_scoring_outputs_i[1])
            list_class_ids.append(select_top_scoring_outputs_i[2])
            list_scores.append(select_top_scoring_outputs_i[3])
            
    if mode == 'training':
        # probs, [batch_size, num_anchors, num_object_classes]
        pred_anchor_probs = tf.keras.layers.Concatenate(
            axis=1, name='pred_anchor_probs')(pred_anchor_probs)
        # offsets, [batch_size, num_anchors, 4]
        pred_anchor_offsets = tf.keras.layers.Concatenate(
            axis=1, name='pred_anchor_offsets')(pred_anchor_offsets)
    
        # cls loss
        cls_inputs = [
            gt_anchor_indicators, gt_anchor_class_ids, pred_anchor_probs]
        cls_loss = losses.ClsLoss(alpha, gamma)(cls_inputs)
        # reg loss
        reg_inputs = [
            gt_anchor_indicators, gt_anchor_offsets, pred_anchor_offsets]
        reg_loss = losses.RegLoss()(reg_inputs)
        
        # training model's inputs and outputs
        inputs = [
            images, 
            gt_anchor_indicators, 
            gt_anchor_class_ids,
            gt_anchor_offsets,]
        outputs = [
            pred_anchor_probs, 
            pred_anchor_offsets, 
            cls_loss, 
            reg_loss]
        
    else:
        # NMS
        nms_fpn_inputs = [
            list_anchor_idxes, list_anchors, list_class_ids, list_scores]
        nms_fpn_outputs = detections.NMS_FPN(
            max_objects_per_class_per_img, 
            iou_threshold,
            batch_size,
            name='nms'
            )(nms_fpn_inputs)
        # anchors_batch, class_ids_batch, scores_batch = nms_fpn_outputs
        
        # inference model's inputs and outputs
        inputs = [images, anchors_fpn_batches, window]
        if output_top_scoring:
            outputs = [nms_fpn_inputs, nms_fpn_outputs]
        else:
            outputs = nms_fpn_outputs
            
    with tf.device('/cpu:0'):
        model = tf.keras.Model(inputs, outputs, name='RetinaNet')
        return model
        
        

