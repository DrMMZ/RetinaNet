"""
@author: Ming Ming Zhang, mmzhangist@gmail.com

Focal and Smooth-L1 Losses
"""

import tensorflow as tf


def sigmoid_focal_loss(
        y_true, 
        y_pred, 
        alpha=0.25, 
        gamma=2.0, 
        from_logits=False):
    """
    Computes the sigmoid focal loss, with class balancing parameter. Adapted from 
    https://www.tensorflow.org/addons/api_docs/python/tfa/losses/SigmoidFocalCrossEntropy

    Parameters
    ----------
    y_true : tensor
        Targets of any shape.
    y_pred : tensor
        Predictions of same shape as y_true.
    alpha : float, optional
        A weighting factor in [0,1] for the positive class, addressing class 
        imbalance. The default is 0.25.
    gamma : float, optional
        A focusing parameter >= 0 for removing easy examples. The default is 2.0.
    from_logits : boolean, optional
        Whether y_pred is a logits tensor. The default is False, i.e., 
        probability.

    Returns
    -------
    loss : tensor
        Focal loss, sumed over all elements from the last dim of y_true, i.e., 
        same shape as y_true without the last dim.

    """
    y_pred = tf.convert_to_tensor(y_pred)
    y_true = tf.convert_to_tensor(y_true)
    y_true = tf.cast(y_true, dtype=y_pred.dtype)
    
    # binary cross-entropy loss
    ce = tf.keras.backend.binary_crossentropy(
        y_true, y_pred, from_logits=from_logits)
    
    # sigmoid estimated probability
    if from_logits:
        pred_prob = tf.math.sigmoid(y_pred)
    else:
        pred_prob = y_pred 
        
    # corresponding t-th probability to y_true
    p_t = (y_true * pred_prob) + ((1 - y_true) * (1 - pred_prob))
    
    # modulating factor
    gamma = tf.convert_to_tensor(gamma, dtype=y_true.dtype)
    modulating_factor = tf.math.pow((1.0 - p_t), gamma)
    
    # alpha factor
    alpha = tf.convert_to_tensor(alpha, dtype=y_true.dtype)
    alpha_factor = (y_true * alpha) + ((1 - y_true) * (1 - alpha))
    
    # focal loss, loss.shape == y_true.shape
    loss = alpha_factor * modulating_factor * ce
    
    # sum over all elements in the last dim
    loss = tf.math.reduce_sum(loss, axis=-1)
    return loss


class SigmoidFocalLoss(tf.keras.losses.Loss):
    """
    Defines a sigmoid focal loss layer as a subclass of TF loss.
    
    """
    def __init__(
            self, 
            alpha=0.25, 
            gamma=2.0, 
            from_logits=False, 
            reduction=tf.keras.losses.Reduction.NONE
            ):
        super().__init__(reduction=reduction, name='sigmoid_focal_loss')
        self.alpha = alpha
        self.gamma = gamma
        self.from_logits = from_logits
        
    def call(self, y_true, y_pred):
        loss = sigmoid_focal_loss(
            y_true, 
            y_pred, 
            alpha=self.alpha, 
            gamma=self.gamma, 
            from_logits=self.from_logits
            )
        return loss
    
    
def cls_loss_fn(
        gt_anchor_indicators, 
        gt_anchor_class_ids, 
        pred_anchor_probs,
        alpha=0.25,
        gamma=2.0
        ):
    """
    Computes the classification loss of RetinaNet.

    Parameters
    ----------
    gt_anchor_indicators : tf tensor, [batch_size, num_anchors]
        Ground-truth anchors indicate negative -1, neutral 0 and positive 1 
        anchors, generated by anchors.anchors_targets().
    gt_anchor_class_ids : tf tensor, [batch_size, num_anchors, num_object_classes]
        Ground-truth anchor class ids, generated by anchors.anchors_targets(). 
        Note that rows with all zeros indicate negative anchors, i.e., 
        background.
    pred_anchor_probs : tf tensor, [batch_size, num_anchors, num_object_classes]
        Predicted objectness probabilities.
    alpha : float, optional
        A weighting factor in [0,1] for the positive class, addressing class 
        imbalance used in focal loss. The default is 0.25.
    gamma : float, optional
        A focusing parameter >= 0 for removing easy examples used in focal loss. 
        The default is 2.0.

    Returns
    -------
    loss : float
        The focal loss averaged over all positive gt anchors.

    """
    # positive anchors, [num_positive_anchors, 2] where 2 is 
    # (batch_idx, anchor_idx), that are anchors assigned to a gt box
    pos_anchor_idxes = tf.where(gt_anchor_indicators == 1)
    num_pos_anchors = tf.shape(pos_anchor_idxes)[0]
    num_pos_anchors = tf.cast(num_pos_anchors, pred_anchor_probs.dtype)
    # loss, [batch_size, num_anchors]
    loss = sigmoid_focal_loss(
        y_true=gt_anchor_class_ids, 
        y_pred=pred_anchor_probs, 
        alpha=alpha, 
        gamma=gamma, 
        from_logits=False)
    # loss, scalar, computed as the sum of the focal loss over all anchors 
    loss = tf.math.reduce_sum(loss)
    # loss, scalar, normalized by the number of positive anchors
    loss = loss / num_pos_anchors
    return loss


class ClsLoss(tf.keras.layers.Layer):
    """
    Defines a RetinaNet classification loss layer as a subclass of TF layer.
    
    """
    def __init__(self, alpha=0.25, gamma=2.0):
        super(ClsLoss, self).__init__(name='cls_loss')
        self.alpha = alpha
        self.gamma = gamma
        
    def call(self, inputs):
        gt_anchor_indicators = inputs[0]
        gt_anchor_class_ids = inputs[1]
        pred_anchor_probs = inputs[2]
        return cls_loss_fn(
            gt_anchor_indicators, 
            gt_anchor_class_ids, 
            pred_anchor_probs,
            self.alpha,
            self.gamma)
     
    
def smooth_l1_loss(y_true, y_pred):
    """
    Computes the smooth-L1 loss.

    Parameters
    ----------
    y_true : tensor
        Ground-truth targets of any shape.
    y_pred : tensor
        Estimates of same shape as y_true.

    Returns
    -------
    loss : tensor
        The loss, sumed over all elements from the last dim of y_true, i.e., 
        same shape as y_true without the last dim.

    """
    y_pred = tf.convert_to_tensor(y_pred)
    y_true = tf.convert_to_tensor(y_true, dtype=y_pred.dtype)
    
    diff = tf.math.abs(y_true - y_pred)
    less_than_one = tf.cast(tf.math.less(diff, 1.0), y_pred.dtype)
    
    # smooth l1 loss, loss.shape == y_true.shape
    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)
    
    # sum over all elements in the last dim
    loss = tf.math.reduce_sum(loss, axis=-1)
    return loss


class SmoothL1Loss(tf.keras.losses.Loss):
    """
    Defines a smooth-L1 loss layer as a subclass of TF loss.
    
    """
    def __init__(self, reduction=tf.keras.losses.Reduction.NONE):
        super().__init__(reduction=reduction, name='smooth_l1_loss')
        
    def call(self, y_true, y_pred):
        loss = smooth_l1_loss(y_true, y_pred)
        return loss
    
    
def reg_loss_fn(
        gt_anchor_indicators, 
        gt_anchor_offsets, 
        pred_anchor_offsets
        ):
    """
    Computes the regression loss of RetinaNet.

    Parameters
    ----------
    gt_anchor_indicators : tf tensor, [batch_size, num_anchors]
        Ground-truth anchors indicate negative -1, neutral 0 and positive 1 
        anchors, generated by anchors.anchors_targets().
    gt_anchor_offsets : tf tensor, [batch_size, num_anchors, 4]
        Ground-truth anchor offsets, where 4 is center coordinates (y, x, h, w) 
        and generated by anchors.anchors_targets().
    pred_anchor_offsets : tf tensor, [batch_size, num_anchors, 4]
        Predicted offsets.

    Returns
    -------
    loss : float
        The smooth-L1 loss averaged over all positive anchors.

    """
    # positive anchors, [num_positive_anchors, 2] where 2 is 
    # (batch_idx, anchor_idx), that are anchors assigned to a gt box
    pos_anchor_idxes = tf.where(gt_anchor_indicators == 1)
    # [num_positive_anchors, 4] for gt_anchor_offsets and pred_anchor_offsets
    gt_anchor_offsets = tf.gather_nd(gt_anchor_offsets, pos_anchor_idxes)
    pred_anchor_offsets = tf.gather_nd(pred_anchor_offsets, pos_anchor_idxes)
    # loss, [num_positive_anchors, ]
    loss = smooth_l1_loss(
        y_true=gt_anchor_offsets, 
        y_pred=pred_anchor_offsets)
    # loss, scalar
    loss = tf.math.reduce_mean(loss)
    return loss


class RegLoss(tf.keras.layers.Layer):
    """
    Defines a RetinaNet regression loss layer as a subclass of TF layer.
    
    """
    def __init__(self):
        super(RegLoss, self).__init__(name='reg_loss')
        
    def call(self, inputs):
        gt_anchor_indicators = inputs[0]
        gt_anchor_offsets = inputs[1]
        pred_anchor_offsets = inputs[2]
        return reg_loss_fn(
            gt_anchor_indicators, 
            gt_anchor_offsets, 
            pred_anchor_offsets)
